{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National tourism promotion\n",
    "## Group project\n",
    "### This notebook uses the *EuropeTop100Attractions_ENG_20190101_20210821* and the *Holidays.csv* datasets\n",
    "\n",
    "(c) Nuno António 2021 - Rev. 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *EuropeTop100Attractions_ENG_20190101_20210821* dataset description\n",
    "\n",
    "### Sheet **Reviews**\n",
    "Reviews published in Tripadvisor from January 1, 2019 to August 21, 2021, in English, for the top 100 tourist attractions in Europe.\n",
    "- **localID**: string - ID of the attraction\n",
    "- **extractionDate** - date - date when the review was extracted\n",
    "- **globalRating** - numeric - global rating of the attraction at the time of the review extraction (reviews in Tripadvidor are in a scale from 1 to 5 stars)\n",
    "- **positionOnRanking** - numeric - position in TripAdvisor's regional ranking at the extraction date\n",
    "- **sitesOnRanking** - numeric - total number of attractions in TripAdvisor's regional ranking at the extraction date\n",
    "- **totalReviews** - numeric - total reviews written for the attraction at the time of the review extraction\n",
    "- **userName** - string - user name of the TripAdvisor user who posted the review. The user name is composed of two parts (first@second). The first is the public name of the user. The second is the TripAdvisor unique identifier of the user.\n",
    "- **userLocation** - string - location of where the user who posted the review lives. This is not a mandatory field, so many users to not provide their location\n",
    "- **userContributions** - numeric - how many reviews have the user wrote in TripAdvisor at the moment of the extraction of the review\n",
    "- **tripType** - string - type of trip type. This is not a mandatory field\n",
    "- **reviewWritten** - date - date when the review was published\n",
    "- **reviewVisited** - date - date when the customer visited the attraction. The day is always 1 because Tripadvisor only ask users to describe the year and the month, not the day\n",
    "- **reviewRating** - numeric - quantitative rating assigned by the user (1 star - bad to 5 stars - excellent)\n",
    "- **reviewLanguage** - string - language the review was written (in this case should be always \"en\" for english)\n",
    "- **reviewFullText** - string - full text of the review (since this course does not address Text Mining the use of this field is completely optional and its use will not be considered for grading)\n",
    "\n",
    "\n",
    "### Sheet **Attractions**\n",
    "Information about the attractions.\n",
    "- **ID**: string - ID of the attraction\n",
    "- **Name**: string - name of the attraction\n",
    "- **Country**: string - name of the country or region\n",
    "- **ISO**: string - ISO code of the country or region\n",
    "\n",
    "## *Holidays.csv* dataset description\n",
    "Worldwide public holidays. Additional information available at https://docs.microsoft.com/en-us/azure/open-datasets/dataset-public-holidays?tabs=azure-storage#data-access\n",
    "- **countryOrRegion**: string - country or region full name\n",
    "- **countryOrRegionCode**: string - country or region in ISO format\n",
    "- **date**: date - date of the holiday\n",
    "- **holidayName**: string - full name of the holiday\n",
    "- **isPaidTimeOff**: boolean - indicates whether most people have paid time off on this date (only available for US, GB, and India now). If it is NULL, it means unknown\n",
    "- **normalizeHolidayName**: normalized name of the holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group details\n",
    "- Composed of three students. Groups of two are aceptable, but must be approved by instructors.\n",
    "- Students can be from different theory and practical classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work description\n",
    "\n",
    "### Overview\n",
    "<p>Tourism was hit hard by the COVID-19 pandemic. National Tourism Board Organizations (NTBO) want to study users' generated content, such as Social Media reviews, to understand visitors patterns and if these patterns were affected by the pandemic.</p>\n",
    "<p>Employing the CRISP-DM process model, your group are expected to act as a consultant for the Portuguese NTBO. Your job is to characterize and describe the patterns of visitants of Portuguese attractions and comparing it to Portugal's main tourism competitors. In addition, you can try to understand visitors' frequent itemset associations, similarities between attractions or visitors, or segment visitors using RFM (replacing Monetization by ratings for example).</p>\n",
    "\n",
    "Examples of the type of analysis that can be done:\n",
    "- Comparison of reviews frequency and rating distribution before and after the pandemic\n",
    "- Comparison of the type of trip and origin of visitors\n",
    "\n",
    "<p>Invest time in explaining the rationale of your choices and your business recommendations based on your findings.</p>\n",
    "\n",
    "### Deliverables\n",
    "- Python source code (Jupyter notebook or .py files) and/or Excel files. Python code should be commented to facilitate comprehension\n",
    "- Powerpoint presentation\n",
    "- Report:\n",
    "    - Maximum of 20 pages (excluding appendixes)\n",
    "    - Minimum font size is 10\n",
    "    - Should describe the main outputs according to CRISP-DM, including the brief description of the problem, methods, results, and their discussion\n",
    "\n",
    "\n",
    "### Presentation\n",
    "- To be done in the exam season with all group members present\n",
    "- Slots of 20 minutes per group\n",
    "- 10 minutes for presentation, another 10 for discussion\n",
    "\n",
    "\n",
    "### Questions or additional informations\n",
    "For any additional questions, don't hesitate to get in touch with the instructors of the practical classes. They will also act as the national tourism board business/project stakeholders.\n",
    "\n",
    "<br><br>\n",
    "Good work or good luck ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING (business objectives, assess situation, DM goals, Produce project plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business understanding section explained in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING & UNDERSTANDING (collect, describe, explore, verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sea\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import joypy\n",
    "import re\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import networkx as nx\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import ipympl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import squarify\n",
    "a4_dims = (20, 12)\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# import of libraries needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subPlots_Title_fontSize = 12\n",
    "subPlots_xAxis_fontSize = 10\n",
    "subPlots_yAxis_fontSize = 10\n",
    "subPlots_label_fontSize = 10\n",
    "heatmaps_text_fontSize = 8\n",
    "\n",
    "plots_Title_fontSize = 14\n",
    "plots_Title_textColour = 'black'\n",
    "\n",
    "plots_Legend_fontSize = 12\n",
    "plots_Legend_textColour = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attraction data \n",
    "ds_attractions = pd.read_excel('EuropeTop100Attractions.xlsx')\n",
    "\n",
    "# This excel is provided by the team via moodle submission\n",
    "# We split each sheet from the 100attractions excel file into separate \"excel\" - so the neccessary merging of tables we do here in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, prior loading attraction data above - our team has made a quick checkup of data accuracy through pivot tables. \n",
    "\n",
    "Some basic crosschecks of duplicated names and country names have been investigated and corrected. Following actions have been done just to save time and effort that can be used in further analysis and data cleaning. Please, follow the actions we have taken prior ds_attractions:\n",
    "\n",
    "- 2x same name \"Old Town\" --> treated as Warsaw and Dubrownik Old Towns after double check through reviews escel sheet\n",
    "- MAG045 (Old town) that has been renamed to Warsaw Old Town has been given wrong ISO (HR) instead of PL. This has been corrected.\n",
    "- Vatican has been assigned to italy but with VA ISO code and not italy. We changed it to Italian ISO code\n",
    "- Scot and Scotland were like two countries, even though they are obviously one. We changed Scot to Scotland. With this, as Scotland belongs to the UK, we expect from the data description to have more countries than ISO codes. UK ISO code can hold England too for ex.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming ID column to the same column name as in other excel file to have a succesfull merge of two sheets\n",
    "ds_attractions.rename(columns = {'ID':'localID'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, Name, Country, ISO]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether Old Town is not duplicated anymore since the analysis (through pivot table in Excel showed two equal values one in Warsaw and one in Durbovnik with the same respective names)\n",
    "ds_attractions[ds_attractions['Name'] == 'Old Town']\n",
    "\n",
    "# this, as explained before, has been issued in the given excel sheet - this is just a true crosscheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAG001</td>\n",
       "      <td>Basilica of the Sagrada Familia</td>\n",
       "      <td>Spain</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAG002</td>\n",
       "      <td>Parc Guell</td>\n",
       "      <td>Spain</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAG003</td>\n",
       "      <td>Tower of London</td>\n",
       "      <td>England</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAG004</td>\n",
       "      <td>Casa Batllo</td>\n",
       "      <td>Spain</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAG005</td>\n",
       "      <td>Staromestske namesti</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>CZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                             Name         Country ISO\n",
       "0  MAG001  Basilica of the Sagrada Familia           Spain  ES\n",
       "1  MAG002                       Parc Guell           Spain  ES\n",
       "2  MAG003                  Tower of London         England  UK\n",
       "3  MAG004                      Casa Batllo           Spain  ES\n",
       "4  MAG005             Staromestske namesti  Czech Republic  CZ"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_attractions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews sheet data\n",
    "ds_reviews = pd.read_excel('EuropeTop100Attractions_reviews.xlsx')\n",
    "\n",
    "# this excel file is also provided by our team via moodle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bf2012e66ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merging two sheets of the excel file into the one based on common ID - localID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_attractions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# source: https://towardsdatascience.com/left-join-with-pandas-data-frames-in-python-c29c85089ba4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Merging two sheets of the excel file into the one based on common ID - localID\n",
    "ds_one = ds_reviews.merge(ds_attractions, on='localID', how='left')\n",
    "\n",
    "# source: https://towardsdatascience.com/left-join-with-pandas-data-frames-in-python-c29c85089ba4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_one.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General observations: \n",
    "- 92120 rows (records/ratings)\n",
    "- We have some missing values on User Location, triptypes, Name (together witg Country and ISO)\n",
    "- why only 98 unique names --> should be 100 as Local IDs\n",
    "- ISO does not match as country Scotland belongs to the UK, same as England\n",
    "- Too many user locations\n",
    "\n",
    "more in depth explanation provided in the report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_one[\"Country\"].value_counts(ascending=False).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most rated countries - ordered ascendingly (count)\n",
    "plt.figure(figsize=(15,7))\n",
    "ax = sns.countplot(x='Country', data=ds_one, order=pd.value_counts(ds_one['Country']).iloc[:25].index);\n",
    "plt.title('Nr of ratings per country')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 visited (reviewed) attractions\n",
    "plt.figure(figsize=(15,7))\n",
    "ax = sns.countplot(x='Name', data=ds_one, order=pd.value_counts(ds_one['Name']).iloc[:20].index);\n",
    "plt.title('Top 20 most visited places in Europe')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw triptypes across all countries in ascending value\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(x='tripType', data=ds_one, order=pd.value_counts(ds_one['tripType']).iloc[:20].index);\n",
    "plt.title('Trip types ordered ascendignly')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top user locations (where users come from based on nr of ratings provided)\n",
    "plt.figure(figsize=(15,7))\n",
    "ax = sns.countplot(x='userLocation', data=ds_one, order=pd.value_counts(ds_one['userLocation']).iloc[:50].index);\n",
    "plt.title('Most user locations')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# London wins by far, yet we can see that userlocation is so broken based on hundreds of values. Yet, it can be visible that most from users come from either UK or US or Australia. We will go into that more deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist (count)plot on the whole dataset with reviewWritten\n",
    "\n",
    "sns.displot(data=ds_one, x='reviewWritten', kind='hist', height=6, aspect=1.4, bins=35)\n",
    "\n",
    "# setting customized ticklabels for x axis\n",
    "pos = ['2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01',\n",
    "      '2019-10-01','2019-11-01','2019-12-01','2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01'\n",
    "      ,'2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "      '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "  \n",
    "lab = ['Jan19',\n",
    "      'Feb19', 'Mar19', 'Apr19', 'May19', 'June19', \n",
    "       'July19', 'Aug19', 'Sept19', 'Oct19', 'Nov19', 'Dec19','Jan20','Feb20', 'Mar20', 'Apr20', 'May20', 'June20', \n",
    "       'July20', 'Aug20', 'Sept20', 'Oct20', 'Nov20', 'Dec20','Jan21','Feb21', 'Mar21', 'Apr21', 'May21', 'June21', \n",
    "       'July21', 'Aug21']\n",
    "\n",
    "plt.xticks( pos, lab)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# There is a huge drop after Feb2020, meaning Covid has probably hit hard visitance of attractions across all places.\n",
    "# We explore that further only for Portugal and its closest countrioes (Spain, France, italy) through density plot (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSITY PLOT (Kernel Density Estimate) Portugal vs Rest of countries\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "# sns.kdeplot(portugal_two[portugal_two['Updated_location']=='United States']['reviewVisited'], shade=False, color='yellow', x='reviewVisited')\n",
    "# sns.kdeplot(portugal_two[portugal_two['Updated_location']=='United Kingdom']['reviewVisited'], shade=False, color='red', x='reviewVisited')\n",
    "sns.kdeplot(ds_one[ds_one['Country']=='Portugal']['reviewWritten'], shade=False, color='green', x='reviewVisited')\n",
    "sns.kdeplot(ds_one[ds_one['Country']!='Portugal']['reviewWritten'], shade=False, color='blue',x='reviewVisited')\n",
    "\n",
    "pos = [ '2017-10-01', '2018-02-01', '2018-03-01', '2018-04-01', \n",
    "       '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n",
    "       '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n",
    "      '2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01',\n",
    "      '2019-10-01','2019-11-01','2019-12-01','2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01'\n",
    "      ,'2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "      '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "  \n",
    "lab = [ 'Oct17', 'Feb18', 'Mar18', 'Apr18', 'May18', 'June18', \n",
    "       'July18', 'Aug18', 'Sept18', 'Oct18', 'Nov18', 'Dec18', 'Jan19',\n",
    "      'Feb19', 'Mar19', 'Apr19', 'May19', 'June19', \n",
    "       'July19', 'Aug19', 'Sept19', 'Oct19', 'Nov19', 'Dec19','Jan20','Feb20', 'Mar20', 'Apr20', 'May20', 'June20', \n",
    "       'July20', 'Aug20', 'Sept20', 'Oct20', 'Nov20', 'Dec20','Jan21','Feb21', 'Mar21', 'Apr21', 'May21', 'June21', \n",
    "       'July21', 'Aug21']\n",
    "  \n",
    "plt.xticks( pos, lab)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Easy to notice, both Portugal and rest of countries have had the same drop of ratings - meaning COVID has impacted probably all countries strongly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr of unique users (user names) in the dataset\n",
    "ds_one['userName'].nunique()\n",
    "\n",
    "# from more than 90K records, it is clear that we have some users that have visited more than one of presented 100 attractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first observations from the exploration part. Yet, it is very important to say that these are very general and some of them confirmed our assumptions (like Covid has had a huge impact on all countries), but also some new like couples triptype has been by far the most popular type of vacation. \n",
    "\n",
    "After dataset's verification of quality and construction the data according our needs (for further analyses), we will present many more insights in regards to Portugal but also other, mostly neighbouring countries. Meaning, another \"exploration\" part (after data construction) will be unveiled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data verification of quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of unique localIDs (resulting from describe report earlier)\n",
    "# found 2 - genis and u which belong to MAG005 and MAG005 and so they will be transformed as such in Data preparation phase\n",
    "\n",
    "ds_one.localID.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked missing values\n",
    "ds_one.isnull().sum()\n",
    "\n",
    "\n",
    "# Many missing values on triptypes and user location but we most likely keep all such observations (for some analyses they may be useful, and for those including such variables, they would be automatically excluded)\n",
    "# At the moment, the ones from Name, country, ISO - dont really matter as another merge will need to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding duplicates now\n",
    "ds_one.duplicated().sum()\n",
    "\n",
    "# not duplicated rows, but lets check if same user name was not giving rating to same place twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_one[ds_one[['Name', 'userName', 'userLocation']].duplicated() == True]\n",
    "\n",
    "# looks like as those, almost 7,5K rows are like that. In the data preparation phase, we only keep last records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it via random username from the table\n",
    "ds_one[ds_one[\"userName\"] == 'michael.t@michaeltast']\n",
    "\n",
    "# we can see that this user has the same user location twice. In fact, we would the only difference is in the extractionDate\n",
    "# Anyways, we will remove these duplicates in the data preparation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION (Select, clean, construct, integrate, format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the dataset, that is going to be prepared for the modelling\n",
    "\n",
    "ds_one.head(5)\n",
    "\n",
    "# We go with the merged dataset - ds_one (attractions + reviews sheet merged together as up until now)\n",
    "# We dont see any reasons (due to certain data limitations) to use the holidays dataset. It may be used once or twice ocassioanly, but not that the dataset would be \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quality report unveiled 3 issues in the dataset: LocalID, Duplicated rows and Missing values\n",
    "# First two we are going to clean/edit inmediately, starting with Local ID\n",
    "\n",
    "ds_one['localID'] = ds_one['localID'].replace(['u'],'MAG006')\n",
    "ds_one['localID'] = ds_one['localID'].replace(['genis'],'MAG005')\n",
    "\n",
    "# However, we this change, we need to perform the merge operation again. TO be seen in the upcoming codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping dupplicated columns before another merge\n",
    "ds_one = ds_one.drop(['Name', 'Country', 'ISO'], axis = 1)\n",
    "ds_one.head(5)\n",
    "\n",
    "# now the ds_one pretends to be like a \"reviews dataset\" from before which we need to merge it with the attractions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL Data collection followed by the data description \n",
    "ds = ds_one.merge(ds_attractions, on='localID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.describe(include='all').T\n",
    "\n",
    "# description shows us improved values like having 100unique names that goes in hand with 100unique LocalIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erased all duplicates with same values for name, username and userlocation - only keeping the \"last\" record\n",
    "ds = ds.drop_duplicates(['Name','userName','userLocation'],keep= 'last')\n",
    "ds.info()\n",
    "\n",
    "# now we can see that our dataset has lost more than 7K duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of no more duplicates acrss all potential variables but it looks all good now\n",
    "ds[ds[['Name', 'userName', 'userLocation','tripType','reviewVisited','tripType','userContributions']].duplicated() == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosscheck whether this is correct. We perform it on the same username via which we demonstrated the appearance of duplicates\n",
    "ds[ds[\"userName\"] == 'michael.t@michaeltast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously stated, we are not going to do anything with the missing values yet, since they appear not to be harmful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the change the order --> of 3 merged columns to be placed at the begining after localID)\n",
    "# + Getting rid of meaningless or duplicated columns (ISO, extractionDate, position on ranking, reviewlanguage, sitesonranking, TotalReviews)\n",
    "ds = ds.loc[:, [\"localID\",\"Name\",\"Country\",\"globalRating\",\"userName\",\"userLocation\",\"userContributions\",\"tripType\",\"reviewWritten\",\"reviewVisited\",\"reviewRating\",\"reviewFullText\"]]\n",
    "\n",
    "# https://towardsdatascience.com/4-methods-for-changing-the-column-order-of-a-pandas-data-frame-a16cf0b58943\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General observations:\n",
    "\n",
    "- 100 attractions visitied and kept in the data set from 25 countries\n",
    "- Spain is te most visited and rated country, with Sagrada being the nr 1 attraction\n",
    "- 3/4 of reviews are written are by people who have only rated once (1 location only)\n",
    "- there are 12670 different user unique locations - Our team anticipates that it may be very hard to analyze this variable\n",
    "- Out of the 2/3 of reviews written with certain trip type, couples are recorded in every second ocassion\n",
    "- The month with the highest record of visitance and revie written in this date set is september 2019 (before covid)\n",
    "\n",
    "Other:\n",
    "- user contributions seem to have an outlier (looking at the Max value) - still will see if we will work with this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding new columns, more specifically:\n",
    "- Covid_time = after / before (based on the data already, but also overall start of the covid spread across Europe we set that everything before March2020 is the time before and everythig after --> after covid\n",
    "- visits_together = each row with corresponding country will be assigned total number of visits per country (may be useful later in out analyses and comparison of Portugal to other countries\n",
    "- Day of the week at which the review at certain attraction was given\n",
    "- user visits for specific countries (explained in the report + to be understood from blocks of code later)\n",
    "\n",
    "We also considered\n",
    "- AVG time between attraction visit and review written but as one variable is with month and another with days, it would not be that accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new column whether review visitied was before or after covid\n",
    "ds.loc[ds['reviewVisited'] >= '2020-03-01', 'Covid_time'] = 'After Covid'\n",
    "ds.loc[ds['reviewVisited'] < '2020-03-01', 'Covid_time'] = 'Before Covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new column with total visits per each country \n",
    "ds['visits_together_per_country'] = ds['Country'].map(ds['Country'].value_counts())\n",
    "ds.head(5)\n",
    "\n",
    "# https://stackoverflow.com/questions/17709270/create-column-of-value-counts-in-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of the week - review given\n",
    "ds['day_of_week_review_given'] = ds['reviewWritten'].dt.day_name()\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create separate dataset for:\n",
    "- Portugal\n",
    "- Spain\n",
    "- Italy\n",
    "- France\n",
    "- Portugal + Italy + Spain + France as all these countries have high representation of records (more than 4000 each) and are most visited countries in Europe (source: https://www.schengenvisainfo.com/travel-guide/top-10-most-visited-european-countries/) - Portugal can be inspired and have the highest benchmark. Also culturally, these countries are the most similar ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of mentioned dataset\n",
    "Portugal = ds.apply(lambda row: row[ds['Country'].isin(['Portugal'])])\n",
    "italy = ds.apply(lambda row: row[ds['Country'].isin(['Italy'])])\n",
    "spain = ds.apply(lambda row: row[ds['Country'].isin(['Spain'])])\n",
    "france = ds.apply(lambda row: row[ds['Country'].isin(['France'])])\n",
    "\n",
    "pr_it_es_fr = ds.apply(lambda row: row[ds['Country'].isin(['Portugal','France','Italy','Spain'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates on username and user location (this one is not used in the analysis though)\n",
    "Portugal_unique_visits = Portugal.drop_duplicates(['userName','userLocation'],keep= 'last')\n",
    "italy_unique_visits = italy.drop_duplicates(['userName','userLocation'],keep= 'last')\n",
    "spain_unique_visits = spain.drop_duplicates(['userName','userLocation'],keep= 'last')\n",
    "france_unique_visits = france.drop_duplicates(['userName','userLocation'],keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding user visits for all countries into separate columns\n",
    "Portugal['user_visits_sum'] = Portugal['userName'].map(Portugal['userName'].value_counts())\n",
    "italy['user_visits_sum'] = italy['userName'].map(italy['userName'].value_counts())\n",
    "spain['user_visits_sum'] = spain['userName'].map(spain['userName'].value_counts())\n",
    "france['user_visits_sum'] = france['userName'].map(france['userName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataset together\n",
    "pr_it_es_fr_visits = [Portugal, italy, spain, france]\n",
    "pr_it_es_fr_visits_final = pd.concat(pr_it_es_fr_visits)\n",
    "pr_it_es_fr_visits_final\n",
    "\n",
    "# https://towardsdatascience.com/joining-datasets-with-pythons-pandas-ed832f01450c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that user location is a key variable to unveils some patters on portuguese visitors, yet as demonstrated earlier, there are so many unique values. Yet, for the portuguese dataset, we try to apply string contain method that could possibly group together majority of user location under common umbrella (like England, Scotland, UK --> all belong to UK). In many ocassions, there is a city and then the country stated (especially US), so we try to apply the method to have statisically significant data from updated user location that we can work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mentioned experiment with the updated user location we will make a copy of a current Portuguese dataset\n",
    "portugal_two = Portugal.copy()\n",
    "portugal_two.isnull().sum()\n",
    "\n",
    "# so we expect 641 records to have \"missing location\" string value under new column \"updated_location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_two['userLocation'] = portugal_two['userLocation'].fillna('Missing location - NaN')\n",
    "\n",
    "# first we fill those missing values with \"Missing location - NaN\" string --> enabling us to these records write under updated location too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column on Portuguese dataset grouped updated location respectively. Those observations that do not match current value we expect to have \"missing values\"\n",
    "\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"UK|Ireland|United Kingdom|England\"), 'Updated_location'] = 'United Kingdom'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"US|United States|CA|ALA|AK|AS|AZ|AR|CA|CO|CT|DE|DC|FL|GA|HU|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|MP|OH|OK|OR|PA|PR|RI|SC|SD|TN|TX|UM|UT|VT|VI|WA|VA|WV|WI|WY\"), 'Updated_location'] = 'United States'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Portugal|portugal\"), 'Updated_location'] = 'Portugal'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Canada|canada\"), 'Updated_location'] = 'Canada'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Australia|New Zealand\"), 'Updated_location'] = 'Australia/New Zealand'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Germany\"), 'Updated_location'] = 'Germany'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Italy\"), 'Updated_location'] = 'Italy'\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Spain\"), 'Updated_location'] = 'Spain'\n",
    "\n",
    "portugal_two.loc[portugal_two['userLocation'].str.contains(\"Missing location - NaN\"), 'Updated_location'] = 'Missing location - NaN'\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/36653419/str-contains-to-create-new-column-in-pandas-dataframe\n",
    "# https://stackoverflow.com/questions/26577516/how-to-test-if-a-string-contains-one-of-the-substrings-in-a-list-in-pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_two.isnull().sum()\n",
    "# we have 1290 locations that have not been matched - so we will assign them a \"different country\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_two['Updated_location'] = portugal_two['Updated_location'].fillna('Different country')\n",
    "portugal_two.isnull().sum()\n",
    "\n",
    "# Again, triptyp missing values do not mind as 2/3 of the rows can be satisfactory when analysing triptypes separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to integrate any other data. We are good here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with Data integration, no formatting is needed for the analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION PART 2 (to obtain insights from Portugal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Portugal \n",
    "# plt.figure(figsize=(15,7))\n",
    "ax = sns.displot(data=portugal_two, x='reviewVisited', kind='hist', height=6, aspect=1.4, bins=35)\n",
    "\n",
    "\n",
    "# setting customized ticklabels for x axis\n",
    "pos = [ '2017-10-01', '2018-02-01', '2018-03-01', '2018-04-01', \n",
    "       '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n",
    "       '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n",
    "      '2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01',\n",
    "      '2019-10-01','2019-11-01','2019-12-01','2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01'\n",
    "      ,'2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "      '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "  \n",
    "lab = [ 'Oct17', 'Feb18', 'Mar18', 'Apr18', 'May18', 'June18', \n",
    "       'July18', 'Aug18', 'Sept18', 'Oct18', 'Nov18', 'Dec18', 'Jan19',\n",
    "      'Feb19', 'Mar19', 'Apr19', 'May19', 'June19', \n",
    "       'July19', 'Aug19', 'Sept19', 'Oct19', 'Nov19', 'Dec19','Jan20','Feb20', 'Mar20', 'Apr20', 'May20', 'June20', \n",
    "       'July20', 'Aug20', 'Sept20', 'Oct20', 'Nov20', 'Dec20','Jan21','Feb21', 'Mar21', 'Apr21', 'May21', 'June21', \n",
    "       'July21', 'Aug21']\n",
    "  \n",
    "plt.xticks( pos, lab)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Clearly seen a huuge drop after March 2020 to almost zero.\n",
    "# Drop after july can explain long term vacations (more than one months form which many may be assigned to July - same happened in Spain and can be observed via density plots below)\n",
    "# lineplot - to explore what has happened between \n",
    "# https://seaborn.pydata.org/generated/seaborn.lineplot.html perhaps based on pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Torre de Belém']['reviewVisited'], shade=False, color='green', x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Quinta da Regaleira']['reviewVisited'], shade=False, color='blue',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Ponte de Dom Luís I']['reviewVisited'], shade=False, color='pink',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Park and National Palace of Pena']['reviewVisited'], shade=False, color='black',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Mosteiro dos Jeronimos']['reviewVisited'], shade=False, color='red',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Cais da Ribeira']['reviewVisited'], shade=False, color='yellow',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Name']=='Bom Jesus do Monte']['reviewVisited'], shade=False, color='purple',x='reviewVisited')\n",
    "\n",
    "\n",
    "# Allmost every attracrtion had same distribution relative to their own count of ratings across the time (outstanding is just cais de ribeira - timeout market)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSITY PLOT (Kernel Density Estimate) Portugal vs its main competitors (spain, italy, france)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "sns.kdeplot(pr_it_es_fr[pr_it_es_fr['Country']=='Portugal']['reviewVisited'], shade=False, color='green', x='reviewVisited')\n",
    "sns.kdeplot(pr_it_es_fr[pr_it_es_fr['Country']=='Spain']['reviewVisited'], shade=False, color='blue',x='reviewVisited')\n",
    "sns.kdeplot(pr_it_es_fr[pr_it_es_fr['Country']=='Italy']['reviewVisited'], shade=False, color='pink',x='reviewVisited')\n",
    "sns.kdeplot(pr_it_es_fr[pr_it_es_fr['Country']=='France']['reviewVisited'], shade=False, color='black',x='reviewVisited')\n",
    "\n",
    "# even though there may be a small drops in case of Portugal and Spain, the trendline is almost the same for each country, meaning each of the countries have been hit almost the same by Covid19 and it is brutally seen through histograms and density plots presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION - INSIGHT 1 (PRT vs other countries - covid impact on increase of ratings (visitance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG rating before and after covid in Portugal\n",
    "pt_crosstab_ratings = pd.crosstab(index=portugal_two['Name'],\n",
    "            columns=portugal_two['Covid_time'],\n",
    "            values=portugal_two['reviewRating'],\n",
    "            aggfunc=np.mean)\n",
    "pt_crosstab_ratings.style.background_gradient(axis=None, low=0.75, high=1.0)\n",
    "\n",
    "\n",
    "# even some nice differences may be found, bear in mind number of observations after covid and whether they can be statiscally significant\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.background_gradient.html\n",
    "# comparing mean of the ratings for each of the attractions before and after covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric impact on each attraction before and after covid \n",
    "\n",
    "pt_crosstab = pd.crosstab(index=portugal_two['Name'],\n",
    "            columns=portugal_two['Covid_time'],\n",
    "            values=portugal_two['reviewRating'],\n",
    "            aggfunc=\"count\")\n",
    "pt_crosstab.style.background_gradient(axis=None, low=0.75, high=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric impact on each attraction before and after - but now os % increase, to see which attractions seem to \"suffer\" less\n",
    "\n",
    "pt_crosstab[\"AC growth after before covid\"] = (pt_crosstab[\"After Covid\"] / pt_crosstab[\"Before Covid\"]) * 100\n",
    "pt_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing absolute number of visit before and after covid\n",
    "pr_es_it_fr_crosstab = pd.crosstab(index=pr_it_es_fr['Country'],\n",
    "            columns=pr_it_es_fr['Covid_time'],\n",
    "            values=pr_it_es_fr['reviewRating'],\n",
    "            aggfunc=\"count\")\n",
    "\n",
    "pr_es_it_fr_crosstab[\"AC growth after before covid\"] = (pr_es_it_fr_crosstab[\"After Covid\"] / pr_es_it_fr_crosstab[\"Before Covid\"]) * 100\n",
    "pr_es_it_fr_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building DF to display AC growth after before covid\n",
    "\n",
    "France = (223/3403) * 100\n",
    "print(France)\n",
    "\n",
    "Italy = (427/5859) * 100\n",
    "print(Italy)\n",
    "\n",
    "portugal = (309/4948) * 100\n",
    "print(portugal)\n",
    "\n",
    "Spain =  698/19752 * 100\n",
    "print(Spain)\n",
    "\n",
    "together_increase = [['Italy', 7.287933094384708], ['France', 6.553041434028799], ['portugal', 6.2449474535165725], ['Spain', 3.5338193600648036]]\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "together_increase_ac = pd.DataFrame(together_increase, columns = ['Country', 'increase after covid (%)'])\n",
    " \n",
    "# print dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart representation of ratings growth after covid\n",
    "\n",
    "together_increase_ac.plot(kind=\"bar\", x='Country', y='increase after covid (%)');\n",
    "plt.title('% of visitance after covid (compared to before covid)')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# clearly, Spain has decreased a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_dims = (6, 4)\n",
    "# fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "order_country = [\"Italy\", \"France\", \"portugal\",\"Spain\"]\n",
    "x = sns.catplot(x=\"Country\", y=\"increase after covid (%)\", kind=\"bar\", data=together_increase_ac, order=order_country, ax=ax)\n",
    "g.fig.suptitle(\"% of visitance after covid (compared to before covid)\", y=1.05)\n",
    "g.set(xlabel=\"Country\", ylabel=\"Increase after covid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION - INISGHT 2 (proportion of visits in Portugal and Spain is far better than in France, Italy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of visitors based on how many visits hey have made in the given country \n",
    "\n",
    "nr_of_visits_crosstab_country = pd.crosstab(pr_it_es_fr_visits_final['Country'],pr_it_es_fr_visits_final['user_visits_sum'],normalize='index')\n",
    "\n",
    "nr_of_visits_crosstab_country = nr_of_visits_crosstab_country.div(nr_of_visits_crosstab_country.sum(1), axis=0)\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "g = nr_of_visits_crosstab_country.plot(kind='barh', stacked=True, ax=ax)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same just in portugal across attractions \n",
    "\n",
    "nr_of_visits_crosstab_pt_attractions = pd.crosstab(Portugal['Name'],Portugal['user_visits_sum'],normalize='index')\n",
    "\n",
    "nr_of_visits_crosstab_pt_attractions = nr_of_visits_crosstab_pt_attractions.div(nr_of_visits_crosstab_pt_attractions.sum(1), axis=0)\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "g = nr_of_visits_crosstab_pt_attractions.plot(kind='barh', stacked=True, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION - INISGHT 3 (Couples being by far the most popular triptypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw triptypes in Portugal in ascending value\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(x='tripType', data=portugal_two, order=pd.value_counts(portugal_two['tripType']).iloc[:20].index);\n",
    "plt.title('Most tripType in Portugal by type')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triptype_pt = pd.crosstab(portugal_two['Name'],portugal_two['tripType'],normalize='index')\n",
    "\n",
    "triptype_pt = triptype_pt.div(triptype_pt.sum(1), axis=0)\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "g = triptype_pt.plot(kind='barh', stacked=True, ax=ax)\n",
    "\n",
    "# clearly couples is the major representation of triptype across all attractions in Poprtugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triptypes proportion across main countries \n",
    "\n",
    "triptype_all = pd.crosstab(pr_it_es_fr['Country'],pr_it_es_fr['tripType'],normalize='index')\n",
    "\n",
    "triptype_all = triptype_all.div(triptype_all.sum(1), axis=0)\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "g = triptype_all.plot(kind='barh', stacked=True, ax=ax)\n",
    "\n",
    "# triptypes across main countries dont differ too much, maybe families are less representated in Portugal than in other countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot in Portugal based on different triptypes\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "\n",
    "sns.kdeplot(portugal_two[portugal_two['tripType']=='Couples']['reviewVisited'], shade=False, color='blue')\n",
    "sns.kdeplot(portugal_two[portugal_two['tripType']=='Friends']['reviewVisited'], shade=False, color='red')\n",
    "sns.kdeplot(portugal_two[portugal_two['tripType']=='Family']['reviewVisited'], shade=False, color='green')\n",
    "sns.kdeplot(portugal_two[portugal_two['tripType']=='Solo']['reviewVisited'], shade=True, color='pink')\n",
    "\n",
    "# Shaded colour is trip type - solo --> proving, this has been the least hit triptype by the covid, yet also very much. However, it confirms assumption of which trip type has beeb affected the least.\n",
    "# Covid clearly hasn't much impacted different triptypes, only solo traveling which makess sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION - INSIGHT 3 - Portuguese as only tourisits (or mayority) of tourists in Portugal after Covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot based on different updated_location (user location) in Portugal\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "# sns.kdeplot(portugal_two[portugal_two['Updated_location']=='United States']['reviewVisited'], shade=False, color='yellow', x='reviewVisited')\n",
    "# sns.kdeplot(portugal_two[portugal_two['Updated_location']=='United Kingdom']['reviewVisited'], shade=False, color='red', x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Canada']['reviewVisited'], shade=False, color='green', x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Portugal']['reviewVisited'], shade=True, color='blue',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Spain']['reviewVisited'], shade=False, color='pink',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Australia/New Zealand']['reviewVisited'], shade=False, color='black',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Different country']['reviewVisited'], shade=False, color='yellow',x='reviewVisited')\n",
    "sns.kdeplot(portugal_two[portugal_two['Updated_location']=='Missing location - NaN']['reviewVisited'], shade=False, color='purple',x='reviewVisited')\n",
    "\n",
    "pos = [ '2017-10-01', '2018-02-01', '2018-03-01', '2018-04-01', \n",
    "       '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n",
    "       '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n",
    "      '2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01',\n",
    "      '2019-10-01','2019-11-01','2019-12-01','2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01'\n",
    "      ,'2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "      '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "  \n",
    "lab = [ 'Oct17', 'Feb18', 'Mar18', 'Apr18', 'May18', 'June18', \n",
    "       'July18', 'Aug18', 'Sept18', 'Oct18', 'Nov18', 'Dec18', 'Jan19',\n",
    "      'Feb19', 'Mar19', 'Apr19', 'May19', 'June19', \n",
    "       'July19', 'Aug19', 'Sept19', 'Oct19', 'Nov19', 'Dec19','Jan20','Feb20', 'Mar20', 'Apr20', 'May20', 'June20', \n",
    "       'July20', 'Aug20', 'Sept20', 'Oct20', 'Nov20', 'Dec20','Jan21','Feb21', 'Mar21', 'Apr21', 'May21', 'June21', \n",
    "       'July21', 'Aug21']\n",
    "  \n",
    "plt.xticks( pos, lab)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION - other observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "ax = sns.countplot(y='day_of_week_review_given', data=portugal_two, order=pd.value_counts(portugal_two['day_of_week_review_given']).iloc[:7].index);\n",
    "\n",
    "# On sundays, people give most ratings but the difference between days is not so much different one from another. Lets have a look into different \"ratings\" habit - to see if we can find some patterns (like on friday, people tend to give better ratings because they feel better ahead of weekend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG rating in Portugal across days when rating is given\n",
    "\n",
    "pt_crosstab_two = pd.crosstab(index=portugal_two['Country'],\n",
    "            columns=portugal_two['day_of_week_review_given'],\n",
    "            values=portugal_two['reviewRating'],\n",
    "            aggfunc=np.mean)\n",
    "pt_crosstab_two.style.background_gradient(axis=None, low=0.75, high=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AVG rating across days when rating is given, segmented by countries\n",
    "\n",
    "countries_crosstab_two = pd.crosstab(index=pr_it_es_fr['Country'],\n",
    "            columns=pr_it_es_fr['day_of_week_review_given'],\n",
    "            values=pr_it_es_fr['reviewRating'],\n",
    "            aggfunc=np.mean)\n",
    "countries_crosstab_two.style.background_gradient(axis=None, low=0.75, high=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - Selecting modeling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out how many users have visitied at least 2 attractions in Portugal - in case there is a high number, we can run association rules\n",
    "Portugal_two_visits = portugal_two[portugal_two['userName'].map(portugal_two['userName'].value_counts()) > 1]\n",
    "Portugal_two_visits['userName'].nunique()\n",
    "\n",
    "# 959 is great number, telling is there is at least 959 combinations of 2+ attractions in the dataset - this is statiscally significant for a try of a association rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we only will go with association rules, also RFM and similarity matrixes are done, however not used in the report and neither would be used for the final presentation to the stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_associations = portugal_two[['Name', 'userName']]\n",
    "portugal_associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table with only portuguese attractions! \n",
    "portugal_associations_pivot = pd.pivot_table(portugal_associations[['userName', 'Name']], index='userName', columns='Name', aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)\n",
    "portugal_associations_pivot\n",
    "\n",
    "# pivot table with all user names --> 1 means the given user visited/gave rating to such place, if 0, it means that no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_frequent_namesets = apriori(portugal_associations_pivot, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the association rules - by support\n",
    "rulesSupport = association_rules(portugal_frequent_namesets, metric=\"support\", min_threshold=0.001)\n",
    "rulesSupport.sort_values(by='support', ascending=False, inplace=True)\n",
    "rulesSupport.head(10)\n",
    "\n",
    "# only torre de belem together with mosteiro de jeronimos are present in 10% of occasions. Other combinations very low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the association rules - by confidence\n",
    "rules_confidence = association_rules(portugal_frequent_namesets, metric=\"confidence\", min_threshold=0.001)\n",
    "rules_confidence.sort_values(by='confidence', ascending=False, inplace=True)\n",
    "rules_confidence.head(10)\n",
    "\n",
    "# now we get to see which once are those ones with higher confidence\n",
    "# Interesting to see, antecedent support is really low (between 2-4%) and those match with either Torre de Belem or Mosteiro whose appeearance in dataset is much more frequent\n",
    "\n",
    "# SO FAR no big insights unveiled, we have suspicion of having dataset that is not representative enoguh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the association rules - by lift\n",
    "rules_lift = association_rules(portugal_frequent_namesets, metric=\"lift\", min_threshold=0.001)\n",
    "rules_lift.sort_values(by='lift', ascending=False, inplace=True)\n",
    "rules_lift.head(10)\n",
    "\n",
    "# not statiscally god enough either :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the length and then displaying combinations that have appeared together in more than 5% of ocassions\n",
    "portugal_frequent_namesets['length'] = portugal_frequent_namesets['itemsets'].apply(lambda x: len(x))\n",
    "# Length=2 and Support>=0.05\n",
    "portugal_frequent_namesets[(portugal_frequent_namesets['length'] == 2) & (portugal_frequent_namesets['support'] >= 0.05)]\n",
    "\n",
    "# nothing breaking is unveiled, these combinations make sense as attractions are close one to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of these rules\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "g = sns.scatterplot(data=rulesSupport, x=\"support\", y=\"confidence\", size=\"lift\", sizes=(20, 200), alpha=0.8)\n",
    "\n",
    "# Decoration\n",
    "sns.despine()\n",
    "plt.title(\"Rules with support above 10% (Lift as size)\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, loc='upper center', \n",
    "          ncol=6, bbox_to_anchor=(0.45, 1.03), frameon=False)\n",
    "\n",
    "# clearly seen 2 patterns\n",
    "# First is as already illustrated that only 2 combinations (of 2 attractions) have support (presence in the dataset) more than 5% - but they also have lower confidence\n",
    "# there are some combinations with low support but high degree of confidence\n",
    "# on the left bottom corner, there are many unrelated attractions combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble plot of the rules based on confidence\n",
    "\n",
    "# Replace frozen sets with strings\n",
    "rules_confidence['antecedents_'] = rules_confidence['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules_confidence['consequents_'] = rules_confidence['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(13,7))\n",
    "ax= plt.scatter(data=rules_confidence, x='consequents_', y='antecedents_', s = rules_confidence['lift']*20, edgecolors = \"red\", c = \"white\", zorder = 2)\n",
    "\n",
    "# Decoration\n",
    "nRules=rules_confidence.shape[0]\n",
    "plt.title(f\"Grouped matrix of the {nRules} rules\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Consequents (RHS)\")\n",
    "plt.ylabel(\"Antecedents (LHS)\")\n",
    "plt.grid(ls = \"--\", zorder = 1)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT! FOLLOWING TWO MODELINGs DONT BELONG TO THE REPORT BUT ARE PRESENTED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateMax = portugal_two.reviewVisited.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add column = totals of visits per each username \n",
    "portugal_rfm = portugal_two.copy()\n",
    "portugal_rfm['user_visits_sum'] = portugal_rfm['userName'].map(portugal_rfm['userName'].value_counts())\n",
    "portugal_rfm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = portugal_rfm.groupby(['userName']).agg(Recency=('reviewVisited', lambda date: (dateMax - date.max()).days),\n",
    "                                   Frequency=('user_visits_sum', 'max'),\n",
    "                                   Monetary=('userContributions', 'max')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinining RFM cutting points based on the quartiles (default for pandas \"describe\")\n",
    "cols = ['Recency','Frequency','Monetary']\n",
    "table = X[cols].describe()\n",
    "table\n",
    "\n",
    "# clear outliers on monetary max\n",
    "# need to investigate recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMF scores\n",
    "\n",
    "# Function\n",
    "def RFMScore(x, col):\n",
    "    if x <= X.quantile(0.25)[col]:\n",
    "        return '1'\n",
    "    elif x <= X.quantile(0.5)[col]:\n",
    "        return '2'\n",
    "    elif x <= X.quantile(0.75)[col]:\n",
    "        return '3'\n",
    "    else:\n",
    "        return '4'\n",
    "\n",
    "# Process\n",
    "X['RScore'] = X['Recency'].apply(RFMScore, col='Recency')\n",
    "X['FScore'] = X['Frequency'].apply(RFMScore, col='Frequency')\n",
    "X['MScore'] = X['Monetary'].apply(RFMScore, col='Monetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with full RMF score and sort the results\n",
    "\n",
    "# Transform to string\n",
    "cols = ['RScore','FScore','MScore']\n",
    "X[cols] = X[cols].astype(str)\n",
    "\n",
    "# Concatenate\n",
    "X['RFMScore'] = X['RScore'] + X['FScore'] + X['MScore']\n",
    "\n",
    "# Sort\n",
    "X = X.sort_values(by=['RFMScore'])\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics per RFM segment\n",
    "RFMStats = X.reset_index().groupby(['RFMScore']).agg(NrUsers=('userName', lambda i: len(i.unique())),\n",
    "                                                     avgRecency=('Recency', 'mean'),\n",
    "                                                     avgFrequency=('Frequency', 'mean'),\n",
    "                                                     avgMonetary=('Monetary', 'mean')).fillna(0)\n",
    "RFMStats.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['RFMScore']=='144']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of RFM\n",
    "%matplotlib inline\n",
    "cols = ['Recency','Frequency','Monetary']\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,4))\n",
    "for var, subplot in zip(X[cols], ax.flatten()):\n",
    "    g = sns.histplot(data=X,\n",
    "                bins=10,\n",
    "                 x=var,\n",
    "                 ax=subplot,\n",
    "                 kde=False)\n",
    "\n",
    "# Decoration\n",
    "sns.despine()\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "fig.suptitle(\"RFM histograms\", fontsize=plots_Title_fontSize);\n",
    "\n",
    "# clear to see that that modeling dataset should have been cleaned by the outliers (monetary for ex. which is user contributions)\n",
    "# also distribution shows that majority of values are close to each other and so, the mean representation of each segment might not differ much, making the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treemap with number of customers by segment\n",
    "\n",
    "# Define colors for levels\n",
    "def assignColor(rfm):\n",
    "    if (rfm=='144'):\n",
    "        hex='#D7D7D7'   # Platinium\n",
    "    elif (rfm in ['142','143','133','134','124']):\n",
    "        hex='#C9B037'   # Gold\n",
    "    elif (rfm in ['141','131','132','122','123','113','114']):\n",
    "        hex='#B4B4B4'   # Silver\n",
    "    else:\n",
    "        hex='#6A3805'   # Bronze\n",
    "    return hex\n",
    "\n",
    "color = [assignColor(x) for x in RFMStats.index]\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "# Plot\n",
    "squarify.plot(sizes=RFMStats['NrUsers'], \n",
    "              label=RFMStats.index,\n",
    "              color = color,\n",
    "              alpha=.9,\n",
    "              pad=True)                    \n",
    "\n",
    "# Decoration\n",
    "plt.title(\"Number of customers by RFM segment\",fontsize=plots_Title_fontSize)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Heatmap\n",
    "\n",
    "# Prepare data\n",
    "tempDF = RFMStats\n",
    "tempDF['Frequency'] = tempDF.index.str[1]\n",
    "tempDF['Monetary'] = tempDF.index.str[2]\n",
    "pt = pd.pivot_table(tempDF, values='avgRecency', \n",
    "                     index=['Frequency'], \n",
    "                     columns='Monetary')\n",
    "\n",
    "# Draw\n",
    "fig , ax = plt.subplots(figsize=(6, 8))\n",
    "heatmap = sns.heatmap(pt,\n",
    "                      square = True,\n",
    "                      linewidths = .5,\n",
    "                      cmap = 'Blues',\n",
    "                      cbar=False,\n",
    "                      fmt='.0f',\n",
    "                      annot = True,\n",
    "                      annot_kws = {'size': heatmaps_text_fontSize+2})\n",
    "\n",
    "# Decoration\n",
    "plt.title(\"Average Recency (days) by Monetary and Frequency levels\", fontsize=plots_Title_fontSize)\n",
    "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMILARITIES / DISIMILARITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_two_pivot_table = pd.pivot_table(portugal_two[['userName', 'Name']], index='userName', columns='Name', aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)\n",
    "portugal_two_pivot_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer similarity matrix\n",
    "customer_customer_sim_matrix = pd.DataFrame(\n",
    "    pairwise_distances(portugal_two_pivot_table,metric='cosine'),\n",
    "    columns = portugal_two_pivot_table.index,\n",
    "    index = portugal_two_pivot_table.index\n",
    ")\n",
    "customer_customer_sim_matrix = customer_customer_sim_matrix.apply(lambda x: 1-x, axis=1) # Transform dissimilarity to similarity\n",
    "customer_customer_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix across attractions\n",
    "product_product_sim_matrix = pd.DataFrame(\n",
    "    pairwise_distances(portugal_two_pivot_table.T,metric='cosine'),\n",
    "    columns = portugal_two_pivot_table.columns,\n",
    "    index = portugal_two_pivot_table.columns\n",
    "    )\n",
    "product_product_sim_matrix = product_product_sim_matrix.apply(lambda x: 1-x, axis=1) # Transform dissimilarity to similarity\n",
    "product_product_sim_matrix\n",
    "\n",
    "# again, the biggest similarity is between Torre de Belem and Mosteiro\n",
    "# other cna be Ponde de Dom Luis and Cais de Riberia\n",
    "# Are here any marketing recommendations based on such analysis???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_similar_items = list(\n",
    "    product_product_sim_matrix\n",
    "        .loc['Park and National Palace of Pena']\n",
    "        .sort_values(ascending=False)\n",
    "        .iloc[1:4]         # 1 to 11 instead of 0 to 10 because the first is the product itself\n",
    "    .index\n",
    ")\n",
    "top_10_similar_items\n",
    "# Based on similarity - once we pick one attraction, the top 3 most similar attractiosn can be spitted out - again, useful for some marketing activities?\n",
    "# this method can be used for collective tickets to the attractions (BCN example with museums) maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python388jvsc74a57bd040d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
